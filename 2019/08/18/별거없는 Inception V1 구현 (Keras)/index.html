<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>draft-dl-1 | Optimize Everything</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="‘별거 없는’ 시리즈는, 논문을 리뷰하고 코드로 구현해 보면서, 정말 논문과 코드 둘다 별거 없다는 것을 보여주기 위한 시리즈입니다. 사실상 대학들에서 나오는 논문들을 보면 쓸모없는 것 90%, 쓸모있는 것 10%라고 생각합니다. (물론 모든 논문은 그 나름의 가치가 있습니다만, 거칠게 말해서 그렇다는 것입니다.) 이 시리즈에서는 쓸모있는 것 10%만 뽑아">
<meta property="og:type" content="article">
<meta property="og:title" content="draft-dl-1">
<meta property="og:url" content="https://nevfiasco.github.io/2019/08/18/별거없는 Inception V1 구현 (Keras)/index.html">
<meta property="og:site_name" content="Optimize Everything">
<meta property="og:description" content="‘별거 없는’ 시리즈는, 논문을 리뷰하고 코드로 구현해 보면서, 정말 논문과 코드 둘다 별거 없다는 것을 보여주기 위한 시리즈입니다. 사실상 대학들에서 나오는 논문들을 보면 쓸모없는 것 90%, 쓸모있는 것 10%라고 생각합니다. (물론 모든 논문은 그 나름의 가치가 있습니다만, 거칠게 말해서 그렇다는 것입니다.) 이 시리즈에서는 쓸모있는 것 10%만 뽑아">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625442.jpg">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625473.png">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625479.png">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625487.png">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625528.png">
<meta property="og:image" content="https://nevfiasco.github.io/images/img-1566120625545.png">
<meta property="og:updated_time" content="2019-08-18T09:34:03.697Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="draft-dl-1">
<meta name="twitter:description" content="‘별거 없는’ 시리즈는, 논문을 리뷰하고 코드로 구현해 보면서, 정말 논문과 코드 둘다 별거 없다는 것을 보여주기 위한 시리즈입니다. 사실상 대학들에서 나오는 논문들을 보면 쓸모없는 것 90%, 쓸모있는 것 10%라고 생각합니다. (물론 모든 논문은 그 나름의 가치가 있습니다만, 거칠게 말해서 그렇다는 것입니다.) 이 시리즈에서는 쓸모있는 것 10%만 뽑아">
<meta name="twitter:image" content="https://nevfiasco.github.io/images/img-1566120625442.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Optimize Everything" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Noto+Sans+KR:100,300,400,700&amp;subset=korean" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <!--<div id="banner"></div>-->
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        <a href="/" id="main-nav-title" class="main-nav-link">Optimize Everything</a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-별거없는 Inception V1 구현 (Keras)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      draft-dl-1
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>‘별거 없는’</strong> 시리즈는, 논문을 리뷰하고 코드로 구현해 보면서, 정말 <strong>논문과 코드</strong> 둘다 별거 없다는 것을 보여주기 위한 시리즈입니다. 사실상 대학들에서 나오는 논문들을 보면 <strong>쓸모없는 것 90%, 쓸모있는 것 10%</strong>라고 생각합니다. (물론 모든 논문은 그 나름의 가치가 있습니다만, 거칠게 말해서 그렇다는 것입니다.) 이 시리즈에서는 쓸모있는 것 10%만 뽑아서, 최대한 쉽게 설명하고, 쉽게 쉽게 넘어가려 합니다. 특정 부분에 있어서는 쉽게 얘기하기 위해서, 논문의 쓸데없는 부분을 무시할 수 있으니 참고 바랍니다.</p>
<hr>
<p><img src="/images/img-1566120625442.jpg" alt="img"></p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="들어가면서"><a href="#들어가면서" class="headerlink" title="들어가면서"></a><strong>들어가면서</strong></h2><p>2014년 논문이다. 그 당시 ImageNet의 INLSVRC 2014에서 Classification과 Detection분야에서 우승을 한 Architecture이다. 이 이후로 V2, V3 등등이 나왔는데, 일단은 V1만 이해하면 다른 것들은 쉽게 이해할 수 있을 것 같다. 보통은 Classification만을 구현해 보는데, 우리는 한번 Detection까지 어떻게 가는지 알아보도록 하자.</p>
<hr>
<h2 id="이-논문의-기여-사항"><a href="#이-논문의-기여-사항" class="headerlink" title="이 논문의 기여 사항"></a><strong>이 논문의 기여 사항</strong></h2><p><strong>1) 리소스/속도 이슈 풀기</strong> : 일단 기본적으로 Deep Learning은 리소스가 많이 사용되고, 속도가 느린 이슈가 있다. 이것을 해결하기 위한 적절한 Depth와 Width를 가진 Architecture를 Inception V1에서 제안한다. (Hebbian Principle을 사용한다.)</p>
<ul>
<li>AlexNet(2년전 것)대비하여 12배 적은 파라미터를 사용했다. 그리고 더 정확하고</li>
<li>2개의 CNN Layer가 연결되어있을때, Filter의 개수가 늘어나면, 그것의 x^2(quadratic)하게 computational power가 증가한다.</li>
<li>대부분의 Weight가 0으로 되었을 때 에는 Filter가 늘더라도 비효율적이다. 단지 Computational Power만 낭비된다.</li>
<li>Google팀은 Arora의 논문(Provable bounds for learning some deep representations) 참고했는데, 이는, 생물학적인 모방결과 dataset의 확률적 분포가 크고 아주 sparse한 deep neural network로 표현될 수 있다면, 최적화된 network 구조가 만들어질 수 있다는 것 이다. (마지막 layer의 activation의 관계성 통계를 Analyze해보고, 높은 관련성을 지닌 출력값과 Neurons들과 Clustering을 해본 결과)<ul>
<li>위의 Arora논문은 수학적으로 명확한 증명이 필요하나, 단순히 말해 Neuron들은 함께 Fire되고 함께 Wire된다는 Hebbian Principle로써 명확화 될 수 있다.</li>
<li>그런데 Sparse한 Model에 있어서, 현재의 Numerical Computation은 비효율 적이다. (Dense한 모델에 효율적으로 만들어져 있다.) 왜냐하면 Lookup과 Cache misses에 대한 Overhead가 존재(Computer Model이 그렇기 때문) 따라서 Dense Algorithm을 쓸 수 밖에 없다. 그리고 현재 Conv는 Dense한 Connection이다.</li>
<li>이렇게 Sparse(이론적 필요)와 Dense(물리적 필요) 사이에서 우리는 무슨 희망을 찾을 수 있을까? 이를 해결하기 위해 Google팀은 아키텍쳐를 큼직한 모듈로써 Sparse하도록 나누고, 반면 모듈 내부는 Dense하게 계산하도록 하였다.</li>
<li>아래의 그림을 보면 위의 것이 Densely Connected Network이고, 아래의 것이 Sparsely Connected Network이다.</li>
</ul>
</li>
</ul>
<p><img src="/images/img-1566120625473.png" alt="img">Densely Connected Network</p>
<p>▼</p>
<p>▼</p>
<p>▼</p>
<p><img src="/images/img-1566120625479.png" alt="img">Sparsely Connected Network</p>
<ul>
<li>NiN에서의 Network안에서의 Network의 특징을 가져옴 (Lin et al)<ul>
<li>공통적으로 1x1 Conv를 쓴다.</li>
<li>차이점으로 NiN은 Network의 표현력을 높이기 위해서였으나, Inception V1에서는 Dimensional Reduction의 목적이 크다.(Computational Bottleneck을 없애려고).  그리고 Network의 Depth만 증가시키는 것이 아니라 Performacne 이슈없이 Width도 키울 수 있다. (보통 Width가 커지만 Performance가 이슈가 발생됨)</li>
</ul>
</li>
</ul>
<p>2) <strong>Overfitting문제</strong> <strong>풀기</strong>: 기존의 크고 Deep한 모델은, 크면 클수록, 깊으면 깊을 수록 Overfitting하는 경향이 컸다. 특히나 Training Data가 적을때 그러한 경향이 컨다. 그리고 역시나 1)처럼 속도의 이슈를 불러 일으킨다.</p>
<hr>
<h2 id="Architecture-특징"><a href="#Architecture-특징" class="headerlink" title="Architecture 특징"></a><strong>Architecture 특징</strong></h2><table>
<thead>
<tr>
<th>특징</th>
<th>설명</th>
</tr>
</thead>
<tbody><tr>
<td>Inference 속도</td>
<td>1.5 billion multiply-adds</td>
</tr>
<tr>
<td>Layer 개수</td>
<td>27개 Layers</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a><strong>Architecture</strong></h1><h3 id="1-Overall"><a href="#1-Overall" class="headerlink" title="1) Overall"></a>1) Overall</h3><p><img src="/images/img-1566120625487.png" alt="img"></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Patch Size / Stride</th>
<th>output size</th>
<th>Params</th>
<th>OPS</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>Convolution</td>
<td>7 x 7 / 2</td>
<td>112 x 112 x 64</td>
<td>2.7K</td>
<td>34M</td>
<td></td>
</tr>
<tr>
<td>Max Pooling</td>
<td>3 x 3 / 2</td>
<td>56 x 56 x 64</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Convolution</td>
<td>3 x 3 / 1</td>
<td>56 x 56 x 192</td>
<td>112K</td>
<td>360M</td>
<td></td>
</tr>
<tr>
<td>Max Pooling</td>
<td>3 x 3 /2</td>
<td>28 x 28 x 192</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Inception (3a)</td>
<td></td>
<td>28 x 28 x 256</td>
<td>159K</td>
<td>128M</td>
<td></td>
</tr>
<tr>
<td>Inception (3b)</td>
<td></td>
<td>28 x 28 x 480</td>
<td>380K</td>
<td>304M</td>
<td></td>
</tr>
<tr>
<td>Max Pooling</td>
<td>3 x 3 / 2</td>
<td>14 x 14 x 480</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Inception (4a)</td>
<td></td>
<td>14 x 14 x 512</td>
<td>364K</td>
<td>73M</td>
<td></td>
</tr>
<tr>
<td>Inception (4b)</td>
<td></td>
<td>14 x 14 x 512</td>
<td>437K</td>
<td>88M</td>
<td></td>
</tr>
<tr>
<td>Inception (4c)</td>
<td></td>
<td>14 x 14 x 512</td>
<td>463K</td>
<td>100M</td>
<td></td>
</tr>
<tr>
<td>Inception (4d)</td>
<td></td>
<td>14 x 14 x 528</td>
<td>580K</td>
<td>119M</td>
<td></td>
</tr>
<tr>
<td>Inception (4e)</td>
<td></td>
<td>14 x 14 x 832</td>
<td>840K</td>
<td>170M</td>
<td></td>
</tr>
<tr>
<td>Max Pooling</td>
<td>3 x 3 / 2</td>
<td>7 x 7 x 832</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Inception (5a)</td>
<td></td>
<td>7 x 7 x 832</td>
<td>1027K</td>
<td>54M</td>
<td></td>
</tr>
<tr>
<td>Inception (5b)</td>
<td></td>
<td>7 x 7 x 1024</td>
<td>1388K</td>
<td>71M</td>
<td></td>
</tr>
<tr>
<td>Average Pooling</td>
<td>7 x 7 / 1</td>
<td>1 x 1 x 1024</td>
<td></td>
<td></td>
<td>즉, Global Average Pooling임. 전체 영역에 대한 Pooling. Output은 결국 Feature의 개수만큼이다.</td>
</tr>
<tr>
<td>Dropout (40%)</td>
<td></td>
<td>1 x 1 x 1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Linear</td>
<td></td>
<td>1 x 1 x 1000</td>
<td>1000K</td>
<td>1M</td>
<td></td>
</tr>
<tr>
<td>Softmax</td>
<td></td>
<td>1 x 1 x 1000</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h3><h3 id="2-Details"><a href="#2-Details" class="headerlink" title="2) Details"></a>2) Details</h3><p><img src="/images/img-1566120625528.png" alt="img">Inception Module (Naive Version)</p>
<ul>
<li>Architecture에서 가장 중요한건, Inception Module이다.</li>
<li>위의 그림은 전체 ARchitecture에서 Inception Module을 설명하고 있다. (위의 것은 Naive버전이다)</li>
<li>Inception Layer는 1x1 Conv, 3x3 Conv, 5x5 Conv Layer들의 Combination이다. 그리고 Concatenate Layer에서 Concatenation을 한 후에(single Output Vector로 만들어서) 다음 단계의 Input으로 활용한다.</li>
<li>위의 Layer에 추가적으로 아래와같은 Layer를 만들 수 있다.</li>
</ul>
<p><img src="/images/img-1566120625545.png" alt="img">Inception Module (Modified)</p>
<ul>
<li>이를 통해서 Depth와 Width를 늘리면서도, Overfitting을 막고, 속도를 빠르게 해 준다.</li>
<li>위와 같이 1x1 Conv Layer를 넣음으로써, Dimensionaly Reduction을 수행한다. 3x3 Max Pooling Layer를 추가한다. 이를 통해서 Another Option을 Inception Layer에 제공한다.</li>
<li>3x3 Max Pooling Layer는 Inception Layer에 또 다른 옵션을 주기 위함이다.</li>
<li>Hebbian Principle - Human Learning에 대한 것<ul>
<li>Neuron은 함께 Firing하며, 함께 Wiring된다.</li>
<li>딥러닝 Layer를 만들때, 각 Layer는 이전 Layer의 정보에 집중합니다. 이전 Layer가 엉망이면..다음 Layer도 엉망입니다.</li>
<li>이는 딥러닝 모델을 만들때, 각 Layer은 이전 Layer에 집중한다는 의미이다.어떠한 Layer가 우리의 Deep Learning Model에서 얼굴의 각 부분들에 집중했다고 가정해 보다. 다음 Layer는 아마도 다른 얼굴의 표현을 구분하기 위해서 전체적인 얼굴에 집중 할 것이라는 의미이다. 따라서 실제적으로 이것을 하기 위하여, Layer는 다른 Object들을 검출하기 위하여, 적절한 Filter Size를 가져야만 한다.</li>
</ul>
</li>
<li>1x1, 3x3, 5x5로 정한 것은 어떠한 연구에 의한 것이 아니라, 단순히 편의성 때문에 선택한 것이다.</li>
<li>Max Pooling도 역시 최근에는 인식률 증가에 의해서 거의 필수적이므로, 단지 한번 추가한 것이다. (이유는 모른다.)</li>
<li>모든 layer에서(Inception Module에서도) Conv다음에 <strong>ReLU</strong>를 다 사용했다.</li>
<li>224x224 RGB 이미지를 사용하고, mean subtration을 사용했다.</li>
<li>Projection Layer는 Max-Pooling다음에 1x1 Conv로써, Dimension을 맞추기 위해 사용했다. (Concatenation을 위하여)</li>
<li>NiN 논문을 차용하여, Average Pooling을 FCN전에 사용했다. (FCL만을 사용했을 때 보다 Top-1 Accuracy에서 0.6%향상을 보였다. 이는 NiN에서 설명되었듯, Adapting과 Fine-Tunning을 가능하게 했다.)</li>
</ul>
<hr>
<h1 id="Training-Method"><a href="#Training-Method" class="headerlink" title="Training Method"></a><strong>Training Method</strong></h1><ul>
<li>이 Architecture를 사용한 목적은, ImageNet Challenge에서 우승하기 위하여, 몇가지 부가적인 조작을 하였다.<ul>
<li>Data Augmentation</li>
<li>Hyperparameter Setting for Challenge</li>
<li>Optimization Technique &amp; Learning Rate Scheduling</li>
<li>Auxiliary Training</li>
<li>Ensembling Techniques</li>
</ul>
</li>
<li>이중에서 나머지는 걍 그렇고, Auxiliary Training은 꽤 흥미롭고 Novel하다. 그래서 이것을 좀 더 깊게 보려고 한다.</li>
<li>중간 Network에 죽지 않도록 하기 위해서, Auxiliary Classifier(2개의 Softmax를 중간 중간에 넣었다)를 넣였다. 그들은 본질적으로 Auxiliary Loss를 같은 Label에 대해서 계산하고, 최종 로스에 Weighted sum으로 합쳐진다. (auxiliary loss * 0.3 + real loss * 0.7)</li>
<li>Inference Time에서 Auxiliaray Network는 제거된다.</li>
<li>여러가지 방법을 썼는데,<ul>
<li>Andrew Howard의 Photometric Distortion사용</li>
<li>Smaller and Larger Relative Crop 사용</li>
<li>Various Sized Patches Sampling사용(8% ~ 100%)</li>
<li>다양한 랜덤 Aspect Ratio(3/4, 4/3 사용)</li>
<li>그리고 다양한 Random Interpolation Method(Bilinear, area, nearest neighbor, cubic 등 )사용</li>
<li>Hyper parameter들 사용 등등</li>
<li>어떠한 방법떄문에 Training이 최종 결과를 잘 낼수 있었는지는 모르겠다…라고 밝힘</li>
</ul>
</li>
<li>Classification에서 Ensemble을 사용하였는데, 총 7개의 모델을 사용(6개는 같고, 1개는 좀 더 Wider함)<ul>
<li>Training시에<ul>
<li>똑같은 초기 Weight값</li>
<li>똑같은 Learning Rate Policy</li>
<li>다른 Sampling Method와 다른 Random Ordering Selection of input images</li>
</ul>
</li>
<li>Testing시에<ul>
<li>좀더 Aggressive한 Cropping Approach를 사용했다. (이미지를 4배 크게 했다. 좀더 짧은 쪽으로), 그리고 왼쪽, 가운데, 오른쪽 들의 Qaure를 Resized이미지로 사용했다.</li>
<li>하나의 이미지를 Classification하기 위해서 144개(4<em>3</em>6*2)의 Crop된 이미지를 사용했다. (OMG)</li>
<li>Softmax Probabilities는 모든 이미지 Crop의 결과에 대해 평균했다.<ul>
<li>Crop들에 대해서 Max Pooliing하고 Classifier에 대해서 Averaging해봤는데, 위의 결과에 대한 단순 평균보다 결과가 좋지 않았다…</li>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Implementation-by-Keras"><a href="#Implementation-by-Keras" class="headerlink" title="Implementation (by Keras)"></a><strong>Implementation (by Keras)</strong></h1><ul>
<li>Cifar-10으로 가지고 Implementation을 해보자. Cifar는 32x32x3의 이미지로 60,000개의 Data가 있고 50,000개는 Training, 10,000개는 Testing Data로 구성되어 있다. Class는 10개 이다. </li>
</ul>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line"># os.environ[&quot;KERAS_BACKEND&quot;] = &apos;plaidml.keras.backend&apos;</span><br><span class="line"></span><br><span class="line">import keras</span><br><span class="line">import keras.backend as K</span><br><span class="line"></span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Input, concatenate, GlobalAveragePooling2D, \</span><br><span class="line">    AveragePooling2D, Flatten</span><br><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from keras.datasets import cifar10</span><br><span class="line">from keras.utils import np_utils</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line">from keras.optimizers import SGD</span><br><span class="line">from keras.callbacks import LearningRateScheduler</span><br><span class="line">import matplotlib</span><br><span class="line"></span><br><span class="line">matplotlib.use(&apos;Agg&apos;)</span><br><span class="line"># Loading dataset and Performing some preprocessing steps.</span><br><span class="line"></span><br><span class="line">num_classes = 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_cifar10_data(img_rows, img_cols):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Cifar Image를 다운로드 받아서</span><br><span class="line">    이미지를 training과 valid로 나누고</span><br><span class="line">    Preprocessing을 해 준다.</span><br><span class="line">    :param img_rows: 리사이징 이미지 크기 (Row)</span><br><span class="line">    :param img_cols: 리사이징 이미지 크기 (Col)</span><br><span class="line">    :return: normalized된 train과 valid 이미지 numpy array</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # load cifar-10 training and validation sets</span><br><span class="line">    (x_train, y_train), (x_valid, y_valid) = cifar10.load_data()</span><br><span class="line">    x_train = x_train[0:2000, :, :, :]</span><br><span class="line">    y_train = y_train[0:2000]</span><br><span class="line"></span><br><span class="line">    x_valid = x_valid[0:500, :, :, :]</span><br><span class="line">    y_valid = y_valid[0:500]</span><br><span class="line"></span><br><span class="line">    # resize training images</span><br><span class="line">    x_train = np.array([cv2.resize(img, (img_rows, img_cols)) for img in x_train[:, :, :, :]])</span><br><span class="line">    x_valid = np.array([cv2.resize(img, (img_rows, img_cols)) for img in x_valid[:, :, :, :]])</span><br><span class="line"></span><br><span class="line">    # transform targets to keras compatible format</span><br><span class="line">    y_train = np_utils.to_categorical(y_train, num_classes)</span><br><span class="line">    y_valid = np_utils.to_categorical(y_valid, num_classes)</span><br><span class="line"></span><br><span class="line">    x_train = x_train.astype(&apos;float32&apos;)</span><br><span class="line">    x_valid = x_valid.astype(&apos;float32&apos;)</span><br><span class="line"></span><br><span class="line">    # preprocess data (영상이미지라서 255.0으로 나눠서 normalize한다)</span><br><span class="line">    x_train = x_train / 255.0</span><br><span class="line">    x_valid = x_valid / 255.0</span><br><span class="line"></span><br><span class="line">    return x_train, y_train, x_valid, y_valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># define inception v1 architecture</span><br><span class="line">def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name=None,</span><br><span class="line">                     kernel_init=&apos;glorot_uniform&apos;, bias_init=&apos;zeros&apos;):</span><br><span class="line"></span><br><span class="line">    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(x)</span><br><span class="line"></span><br><span class="line">    conv_3x3_reduce = Conv2D(filters_3x3_reduce, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(x)</span><br><span class="line"></span><br><span class="line">    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3_reduce)</span><br><span class="line"></span><br><span class="line">    conv_5x5_reduce = Conv2D(filters_5x5_reduce, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(x)</span><br><span class="line"></span><br><span class="line">    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5_reduce)</span><br><span class="line"></span><br><span class="line">    max_pool = MaxPool2D((3, 3), strides=(1, 1), padding=&apos;same&apos;)(x)</span><br><span class="line"></span><br><span class="line">    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(max_pool)</span><br><span class="line"></span><br><span class="line">    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def decay(epoch, steps=100):</span><br><span class="line">    initial_lrate = 0.01</span><br><span class="line">    drop = 0.96</span><br><span class="line">    epoch_drop = 8</span><br><span class="line">    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epoch_drop))</span><br><span class="line">    return lrate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    print(&apos;Hello World!!&apos;)</span><br><span class="line"></span><br><span class="line">    x_train, y_train, x_valid, y_valid = load_cifar10_data(224, 224)</span><br><span class="line"></span><br><span class="line">    kernel_init = keras.initializers.glorot_uniform()</span><br><span class="line"></span><br><span class="line">    bias_init = keras.initializers.Constant(value=0.2)</span><br><span class="line"></span><br><span class="line">    input_layer = Input(shape=(224, 224, 3))</span><br><span class="line"></span><br><span class="line">    # Layer 1</span><br><span class="line">    x = Conv2D(64, (7, 7), padding=&apos;same&apos;, strides=(2, 2), activation=&apos;relu&apos;, name=&apos;conv_1_7x7/2&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)</span><br><span class="line"></span><br><span class="line">    x = MaxPool2D((3, 3), strides=(2, 2), name=&apos;max_pool_1_3x3/2&apos;, padding=&apos;same&apos;)(x)</span><br><span class="line"></span><br><span class="line">    # Layer 2</span><br><span class="line">    x = Conv2D(192, (3, 3), padding=&apos;same&apos;, strides=(1, 1), activation=&apos;relu&apos;, name=&apos;conv_2_3x3/1&apos;, kernel_initializer=kernel_init, bias_initializer=bias_init)(x)</span><br><span class="line"></span><br><span class="line">    x = MaxPool2D((3, 3), strides=(2, 2), name=&apos;max_pool_2_3x3/2&apos;, padding=&apos;same&apos;)(x)</span><br><span class="line"></span><br><span class="line">    # Layer 3</span><br><span class="line">    x = inception_module(x, 64, 96, 128, 16, 32, 32, name=&apos;inception_3a&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = inception_module(x, 128, 128, 192, 32, 96, 64, name=&apos;inception_3b&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = MaxPool2D((3, 3), strides=(2, 2), name=&apos;max_pool_3_3x3/2&apos;)(x)</span><br><span class="line"></span><br><span class="line">    # Layer 4</span><br><span class="line">    x = inception_module(x, 192, 96, 208, 16, 48, 64, name=&apos;inception_4a&apos;)</span><br><span class="line"></span><br><span class="line">    # Layer 4 - Auxiliary Learning 1</span><br><span class="line">    x1 = AveragePooling2D((5, 5), strides=3, name=&apos;avg_pool_aux_1&apos;)(x)</span><br><span class="line">    x1 = Conv2D(128, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, name=&apos;conv_aux_1&apos;)(x1)</span><br><span class="line">    x1 = Flatten()(x1)</span><br><span class="line">    x1 = Dense(1024, activation=&apos;relu&apos;, name=&apos;dense_aux_1&apos;)(x1)</span><br><span class="line">    x1 = Dropout(0.7)(x1)</span><br><span class="line">    x1 = Dense(10, activation=&apos;softmax&apos;, name=&apos;aux_output_1&apos;)(x1)</span><br><span class="line"></span><br><span class="line">    x = inception_module(x, 160, 112, 224, 24, 64, 64, name=&apos;inception_4b&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = inception_module(x, 128, 128, 256, 24, 64, 64, name=&apos;inception_4c&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = inception_module(x, 112, 144, 288, 32, 64, 64, name=&apos;inception_4d&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line"></span><br><span class="line">    # Layer 4 - Auxiliary Learning 2</span><br><span class="line">    x2 = AveragePooling2D((5, 5), strides=3, name=&apos;avg_pool_aux_2&apos;)(x)</span><br><span class="line">    x2 = Conv2D(128, (1, 1), padding=&apos;same&apos;, activation=&apos;relu&apos;, name=&apos;conv_aux_2&apos;)(x2)</span><br><span class="line">    x2 = Flatten()(x2)</span><br><span class="line">    x2 = Dense(1024, activation=&apos;relu&apos;, name=&apos;dense_aux_2&apos;)(x2)</span><br><span class="line">    x2 = Dropout(0.7)(x2)</span><br><span class="line">    x2 = Dense(10, activation=&apos;softmax&apos;, name=&apos;aux_output_2&apos;)(x2)</span><br><span class="line"></span><br><span class="line">    x = inception_module(x, 256, 160, 320, 32, 128, 128, name=&apos;inception_4e&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = MaxPool2D((3, 3), strides=(2, 2), name=&apos;max_pool_4_3x3/2&apos;)(x)</span><br><span class="line"></span><br><span class="line">    # Layer 5</span><br><span class="line">    x = inception_module(x, 256, 160, 320, 32, 128, 128, name=&apos;inception_5a&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = inception_module(x, 384, 192, 384, 48, 128, 128, name=&apos;inception_5b&apos;, kernel_init=kernel_init, bias_init=bias_init)</span><br><span class="line">    x = GlobalAveragePooling2D(name=&apos;global_avg_pool_5_3x3/1&apos;)(x)</span><br><span class="line">    x = Dropout(0.4)(x)</span><br><span class="line">    x = Dense(10, activation=&apos;softmax&apos;, name=&apos;output&apos;)(x)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layer, [x, x1, x2], name=&apos;inception_v1&apos;)</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    epoch = 25</span><br><span class="line">    initial_lrate = 0.01</span><br><span class="line"></span><br><span class="line">    sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)</span><br><span class="line">    lr_sc = LearningRateScheduler(decay, verbose=1)</span><br><span class="line"></span><br><span class="line">    model.compile(loss=[&apos;categorical_crossentropy&apos;, &apos;categorical_crossentropy&apos;, &apos;categorical_crossentropy&apos;],</span><br><span class="line">                  loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=[&apos;accuracy&apos;])</span><br><span class="line"></span><br><span class="line">    history = model.fit(x_train, [y_train, y_train, y_train], validation_data=(x_valid, [y_valid, y_valid, y_valid]),</span><br><span class="line">                        epochs=epoch, batch_size=20, callbacks=[lr_sc])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a><strong>Questions</strong></h1><ul>
<li>왜 3x3, 5x5, 1x1, Max Pooling을 Inception Module에서 사용했는가? 7x7 등을 사용하면 안되는가?</li>
<li>왜 Max Pooling이후에 1x1로 Feature Map의 개수를 줄여줬는가?</li>
<li>어떻게 Inception Module 안의 Max Pooling은 Inception Module에 Option을 주는가?</li>
<li>어떻게 Inception Module이 각 Filter Size가 자동으로 선택될 수 있을까?</li>
<li>Average Pooling에 대해서 생각 해 보자. (NiN에서 사용함) 장단점은 뭘까?</li>
<li>여기서 사용된, Some Improvements on deep convolutional neural network based image classification(2013), Andrew G. Howard.논문을 보고 Training하는 방법을 생각해보자. (Small Cropping, Larger Cropping Sampling Training)</li>
<li>GlobalAveragePooling2D에 대해서 생각해보자. AveragePooling과의 차이는 무엇일까?</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://nevfiasco.github.io/2019/08/18/별거없는 Inception V1 구현 (Keras)/" data-id="cjzgsg9h800056sufuygho59b" class="article-share-link">공유</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/18/draft-dl-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-caption">최신</span>
      <div class="article-nav-title">
        
          draft-dl-1
        
      </div>
    </a>
  
  
    <a href="/2019/08/18/Test-page/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-caption">이전</span>
      <div class="article-nav-title">Test page</div>
    </a>
  
</nav>

  
</article>

</section>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Nev Fiasco<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> with 
      theme_by <a href="http://hexo.io/" target="_blank">mango</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>